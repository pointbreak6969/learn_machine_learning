{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc431430",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Systems that mimic human brain's ability to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a8c7d",
   "metadata": {},
   "source": [
    "![Deep Learning Models](https://www.artiba.org/Content/Images/types-of-deep-learning-models.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4a696",
   "metadata": {},
   "source": [
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20250703121717652998/1-.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd9590",
   "metadata": {},
   "source": [
    "ANN is a computational model inspired by the structure and functional aspects of biological neural networks. It is the cornerstone of Deep Learning.\n",
    "\n",
    "It consists of large number of simple, highly interconnected processing elements (neurons) working in unison to solve specific problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600e11c",
   "metadata": {},
   "source": [
    "The term \"Deep\" refers to the number of layers through which the data is transformed. Modern networks have hundreds of layers. Each layer extracts progressively higher level features from the raw input.\n",
    "\n",
    "* Shallow Layers: Might detect simple edges or colors\n",
    "* Deep Layers: Might detect complex shapes, textures, or even specific objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0a117",
   "metadata": {},
   "source": [
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20230410104038/Artificial-Neural-Networks.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a55c43",
   "metadata": {},
   "source": [
    "Biological Term   |   Artificial Equivalent\n",
    "* Dendrite:           Input Layer / Incoming Weights\n",
    "* Cell Body:          Summation and Activation\n",
    "* Axon:               Output of the Neuron\n",
    "* Synapse:            Weights (Strength of Connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5c034",
   "metadata": {},
   "source": [
    "**Layer:** A collection of neurons\n",
    "* Input Layer: Receives raw data.\n",
    "* Hidden Layers: Layers between input and output where the \"learning\" happens.\n",
    "* Output Layer: Provides the final prediction\n",
    "\n",
    "- Weights (w) : Parameters that determine the importance of an input signal. They allow the network to \"choose\" which input features are more important.\n",
    "- Bias (b) : An additional parameter that allows the neuron to shift its activation threshold. It provides the flexibility to \"fire\" the neuron even when inputs are low (or stay silent even when they are high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d954b",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/1*upfpVueoUuKPkyX3PR3KBg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f3051",
   "metadata": {},
   "source": [
    "A neuron performs two main operations:\n",
    "1. **Weighted Summation**: It takes all inputs, multiplies them by their respective weights, and adds a bias.\n",
    "2. **Activation**: It passes that sum through a non-linear function.\n",
    "Mathematically, for a set of inputs x1, x2, x3, ....xn:\n",
    "z = (w1x1 + w2x2 + w3x3 + .... + wnxn) + b\n",
    "\n",
    "Or in vector notation: z = W.X + b\n",
    "\n",
    "Where,\n",
    "* W is the vector of weights.\n",
    "* X is the vector of inputs.\n",
    "* b is the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058ad8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [ 0.5 -0.2  0.1]\n",
      "Weights: [ 0.4  0.7 -0.3]\n",
      "Bias: 0.1\n",
      "Linear Sum (z) = 0.1300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def single_neuron_calculation(inputs, weights, bias):\n",
    "    \"\"\"Calculate the linear output (z) of a single neuron\"\"\"\n",
    "    z = np.dot(inputs, weights) + bias\n",
    "    return z\n",
    "\n",
    "inputs = np.array([0.5, -0.2, 0.1])\n",
    "weights = np.array([0.4, 0.7, -0.3])\n",
    "bias = 0.1\n",
    "\n",
    "\n",
    "linear_output = single_neuron_calculation(inputs, weights, bias)\n",
    "print(f\"Inputs: {inputs}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "\n",
    "print(f\"Linear Sum (z) = {linear_output:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd36ac8",
   "metadata": {},
   "source": [
    "# Activation Functions (Sigmoid, ReLU, Tanh)\n",
    "\n",
    "Without activation functions, a neural network is just a big linear regression model. Activation functions introduce non-linearity, allowing the network to learn complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afceb4ef",
   "metadata": {},
   "source": [
    "### 1. Sigmoid Activation Function\n",
    "\n",
    "It maps any value to a range between 0 and 1.\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "\n",
    "It is great for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd4e35d",
   "metadata": {},
   "source": [
    "### 2. Tanh (Hyperbolic Tangent) Activation Function\n",
    "\n",
    "Maps values to a range between -1 and 1.\n",
    "\n",
    "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "\n",
    "Zero-centered. It typically trains faster than Sigmoid because it pushes gradients in both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347671c",
   "metadata": {},
   "source": [
    "### 3. ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "The current \"gold standard\" for hidden layers.\n",
    "\n",
    "$$f(z) = \\max(0, z)$$\n",
    "\n",
    "It is extremely efficient to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17923aa0",
   "metadata": {},
   "source": [
    "![](https://sebastianraschka.com/images/faq/activation-functions/activation-functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5cf9b",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "Forward Propagation is the process where input data is fed forward through the network to generate an output. Each layer passes its output to the next layer as input.\n",
    "\n",
    "\n",
    "**Steps:**\n",
    "For a network with one hidden layer:\n",
    "1. Input Layer (X): Raw features\n",
    "2. Hidden Layer:\n",
    "    * Calculate linear sum: ($Z^{[1]} = W^{[1]}X + b^{[1]}$)\n",
    "    * Calculate activation: $A^{[1]} = g(Z^{[1]})$ (where $g$ is ReLU or Tanh)\n",
    "3. Output Layer:\n",
    "   * Calculate linear sum:  $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$\n",
    "   * Calculate activation: $\\hat{y} = A^{[2]} = \\sigma(Z^{[2]})$ (for classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56183770",
   "metadata": {},
   "source": [
    "**Vectorization**\n",
    "\n",
    "In practice, we don't calculate one neuron at a time. We use Matrix Multiplication to calculate an entire layer for a whole batch of data at once. This is what makes GPUs so powerful for Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83a4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X Shape: (3, 2)\n",
      "Hidden Layer Output (A1) shape: (4, 2)\n",
      "Final Prediction Output (A2): \n",
      " [[0.50002665 0.50005183]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "# Initialize the data(X) - 3 features, 2 samples\n",
    "X = np.array([[1.0, 2.0],\n",
    "            [0.5, 1.1],\n",
    "            [-0.2, 0.4]])\n",
    "\n",
    "\n",
    "# Parameters for Hidden Layer (4 neurons)\n",
    "W1 = np.random.randn(4, 3)*0.01 # 4 neurons, 3 features\n",
    "b1 = np.zeros((4, 1))\n",
    "\n",
    "\n",
    "# Parameters for Output Layer (1 neuron)\n",
    "W2 = np.random.randn(1, 4) * 0.01 # 1 output, 4 features\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "\n",
    "# Forward Pass\n",
    "# Layer 1\n",
    "Z1 = np.dot(W1, X) + b1\n",
    "A1 = relu(Z1)\n",
    "\n",
    "# Layer 2\n",
    "Z2 = np.dot(W2, A1) + b2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "\n",
    "print(f\"Input X Shape: {X.shape}\")\n",
    "print(f\"Hidden Layer Output (A1) shape: {A1.shape}\")\n",
    "print(f\"Final Prediction Output (A2): \\n {A2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa3baf",
   "metadata": {},
   "source": [
    "### Loss Functions (Cost Function): MSE, Cross-Entropy\n",
    "\n",
    "A Loss Function (also called Cost Function) measures how \"wront\" the model's predictions are.\n",
    "\n",
    "#### 1. Mean Squared Error (MSE)\n",
    "Primarily used for Regression problems.\n",
    "$$J = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "Squaring the difference ensures losses are always positive and penalizes larger errors more heavily.\n",
    "\n",
    "\n",
    "#### 2. Binary Cross-Entropy (Log Loss):\n",
    "Primarily used for Binary Classification.\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}\\log(\\hat{y}^{(i)}) + (1 - y^{(i)})\\log(1 - \\hat{y}^{(i)})]$$\n",
    "\n",
    "It uses logarithms to heavily penalize confident but wrong predictions. If y = 1 and the model predicts 0.01, the loss becomes very large.\n",
    "\n",
    "\n",
    "#### 3. Categorical Cross-Entropy:\n",
    "\n",
    "Used for multi-class classification problems with One-Hot encoded labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cd434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.6067\n"
     ]
    }
   ],
   "source": [
    "def compute_mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def compute_cross_entropy(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1-epsilon)\n",
    "    loss = -1/(m) * np.sum(y_true * np.log(y_pred) + (1 - y_true)*np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "y_actual = np.array([1, 0, 1])\n",
    "y_predicted = np.array([0.9, 0.1, 0.2])\n",
    "print(f\"Cross-Entropy Loss: {compute_cross_entropy(y_actual, y_predicted):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18f85b",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is the \"engine\" that allows neural networks to learn. It is the process of calculating the gradient of the loss function with respect to the weights of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b849d78",
   "metadata": {},
   "source": [
    "![](https://serokell.io/files/a0/a05ov1m.Backpropagation_in_NN_pic1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd566f02",
   "metadata": {},
   "source": [
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20250701163824448467/Backpropagation-in-Neural-Network-1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa6e6b",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/1*W7ZPd1tvyi_cIdpoDdX3DA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ab5c8",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/1*DBjWT-lIAUq8m7aZSGj_Ew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd49de",
   "metadata": {},
   "source": [
    "New Weight = Old Weight minus (Learning rate * gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea8633",
   "metadata": {},
   "source": [
    "* Stochastic Gradient Descent (SGD): Uses one sample at a time, compute loss and gradient for that sample, update. Very noisy but can help escape bad local minima; often needs many passes over the data.\n",
    "\n",
    "* Batch Gradient Descent: Compute the loss and gradient on the entire dataset, then do one update. Very stable but slow for large datasets.\n",
    "\n",
    "* Mini-Batch Gradient Descent: Uses a small batch (eg: 32, or 64 samples) per update. It is the balance between stability and speed. One pass over the whole dataset (using many mini-batches) is called one epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf1a67",
   "metadata": {},
   "source": [
    "**Learning Rate**\n",
    "\n",
    "* Too Large: Updates are huge; the loss may bounce around or even increase (divergence)\n",
    "* Too Small: Updates are tiny; training is very slow\n",
    "* In practice, we often use a schedule (e.g. start with 0.01 and reduce over time) or adaptive optimizers that effectively adjust the step size per parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c60a2",
   "metadata": {},
   "source": [
    "### Optimizers: Adam, RMSprop, SGD\n",
    "\n",
    "\n",
    "And optimizer is the rule we use to update the weights given the gradients. Plain gradient descent uses the same learning rate for every parameter. Adaptive optimizers (like Adam) adjust the effective step size per parameter, which often speeds up training and reduces the need to turn the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599feb4c",
   "metadata": {},
   "source": [
    "* Momentum: Instead of using the gradient directly, we keep a running average (like a moving average) of past gradients. $v_t = \\beta v_{t-1} + g_t$, then update $w \\leftarrow w - \\eta v_t$. Typical $\\beta \\approx 0.9$.\n",
    "\n",
    "\n",
    "$g_t$ = gradient at step t\n",
    "\n",
    "\n",
    "* RMSprop: RMSprop keeps running average of the squared gradients and uses it to scale the update. Parameters that usually have large gradients get a smaller effective cheat; Parameters with small gradients get a larger effective map. So the learning rate is adaptive per parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43eede3",
   "metadata": {},
   "source": [
    "**Adam (Adaptive Moment Estimation)** \n",
    "\n",
    "It combines the idea of momentum (smooth direction) and RMSprop (adaptive step size). It maintains two running averages: one for the squared gradient (second moment), and uses bias correction so that early steps aren't too small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb5a4d",
   "metadata": {},
   "source": [
    "$$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\quad \\text{(momentum)} $$\n",
    "$$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\quad \\text{(squared gradients)} $$\n",
    "$$ \\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t} \\quad \\text{(bias correction)} $$\n",
    "$$ w \\leftarrow w - \\eta \\, \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$\n",
    "\n",
    "\n",
    "Typical defaults: $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=10^{-8}$, $\\eta=0.001$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb11b5f",
   "metadata": {},
   "source": [
    "### Regularization Techniques (Dropout, L2 Regularization)\n",
    "\n",
    "\n",
    "Overfitting means the model fits the training data very well but performs poorly on new data (it memorized the training set instead of learning general patterns).\n",
    "\n",
    "\n",
    "Regularization is any technique that discourages overfitting and encourages the model to be simpler or more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6602a838",
   "metadata": {},
   "source": [
    "**Dropout**\n",
    "\n",
    "During training only: at each forward pass we randomly set a fraction p of the neurons in a layer to zero (we \"drop\" them). The remaining neurons are scaled so the total activation level is roughly preserved (e.g. divide by the probability of keeping a neuron). During test time, we use all neurons and no dropout.\n",
    "\n",
    "During training, we randomly turn off some neurons so that the network doesn't depend on any one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2a4c3",
   "metadata": {},
   "source": [
    "**L2 Regularization** (Weight Decay)\n",
    "\n",
    "\n",
    "We add an extra term to the loss that penalizes large weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e2e90",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/1*ozLs-feHr73kJTfKL8figA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c70879",
   "metadata": {},
   "source": [
    "Topic for next class: Implementing Neural Network from Scratch (Numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f45b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AICN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
